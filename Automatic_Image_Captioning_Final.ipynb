{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Automatic Image Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vpn-PDlTzz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "00deafcd-d032-4c31-8a83-0d05e6d1cfbb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrvN7P8yTsH4",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM4bfRwqTsH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "import string\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import argmax\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghjU7VTWTsIO",
        "colab_type": "text"
      },
      "source": [
        "## Extracting the Features using VGG-16 Model and storing it in Pickle File "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb9qbrLETsIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(directory):\n",
        "\tmodel = VGG16()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\tprint(model.summary())\n",
        "\tfeatures = dict()\n",
        "\tfor name in listdir(directory):\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\timage = img_to_array(image)\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be0EyDyWTsIb",
        "colab_type": "text"
      },
      "source": [
        "## Loading the document with descriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_c5nPm4TsId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Skmufn5TsIn",
        "colab_type": "text"
      },
      "source": [
        "## Loading Description and making it a dict "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR33cdVkTsIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsVCvzCOTsI0",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF5NQfYgTsI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p-ZsmKmTsJA",
        "colab_type": "text"
      },
      "source": [
        "## Converting loaded description into vocabulary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IRsUURFTsJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_vocabulary(descriptions):\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xMMTehHTsJM",
        "colab_type": "text"
      },
      "source": [
        "## Saving Description to file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YIXLBsSTsJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCq0aV6TsJZ",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "## load a pre-defined list of photo identifiers  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z3vogcxTsJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SUcpY2NTsJk",
        "colab_type": "text"
      },
      "source": [
        "## Loading Preprocessed Description "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f26MwZegTsJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_clean_descriptions(filename, dataset):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\ttokens = line.split()\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\tdesc = ' '.join(image_desc)\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYS2mlkTsJt",
        "colab_type": "text"
      },
      "source": [
        "## Loading features from pre-trained model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4whXCH9QTsJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_photo_features(filename, dataset):\n",
        "\tall_features = load(open(filename, 'rb'))\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt_1hsexTsJ6",
        "colab_type": "text"
      },
      "source": [
        "## Convert Dictionaries into list of string "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGjm9HDfTsJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KcZaNMATsKE",
        "colab_type": "text"
      },
      "source": [
        "## Create tokens from list "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWf7zre2TsKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8S7SR3ETsKQ",
        "colab_type": "text"
      },
      "source": [
        "## To calculate maximum number of words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3CfHEwdTsKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQMhj3lqTsKb",
        "colab_type": "text"
      },
      "source": [
        "## create sequences of images, input sequences and output words for an image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t9S_taFTsKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn array(X1), array(X2), array(y)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzm0204bTsKr",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmHj9EdoTsKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDDd57TcTsK6",
        "colab_type": "text"
      },
      "source": [
        "## Converting Integer back into word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KOsZHNhTsK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvMftnBSTsLG",
        "colab_type": "text"
      },
      "source": [
        "## Generating a textual for an image "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCCkktyUTsLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\tin_text = 'startseq'\n",
        "\tfor i in range(max_length):\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\tin_text += ' ' + word\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP4QFIQJTsLR",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmjnBHckTsLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF2_ZFQSTsLe",
        "colab_type": "text"
      },
      "source": [
        "## Calling VGG-16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6d-YtxDTsLg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a4ea251-26bc-4e25-e5a3-b55741adba8a"
      },
      "source": [
        "directory = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Images'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "dump(features, open('features.pkl', 'wb'))\n",
        "%time"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            ">99171998_7cc800ceef.jpg\n",
            ">96420612_feb18fc6c6.jpg\n",
            ">93922153_8d831f7f01.jpg\n",
            ">96973080_783e375945.jpg\n",
            ">95728660_d47de66544.jpg\n",
            ">95151149_5ca6747df6.jpg\n",
            ">95734038_2ab5783da7.jpg\n",
            ">96978713_775d66a18d.jpg\n",
            ">95734035_84732a92c1.jpg\n",
            ">97105139_fae46fe8ef.jpg\n",
            ">97406261_5eea044056.jpg\n",
            ">97577988_65e2eae14a.jpg\n",
            ">94232465_a135df2711.jpg\n",
            ">95734036_bef6d1a871.jpg\n",
            ">99679241_adc853a5c0.jpg\n",
            ">95728664_06c43b90f1.jpg\n",
            ">44129946_9eeb385d77.jpg\n",
            ">27782020_4dab210360.jpg\n",
            ">97731718_eb7ba71fd3.jpg\n",
            ">42637986_135a9786a6.jpg\n",
            ">47870024_73a4481f7d.jpg\n",
            ">98377566_e4674d1ebd.jpg\n",
            ">23445819_3a458716c1.jpg\n",
            ">96399948_b86c61bfe6.jpg\n",
            ">61209225_8512e1dad5.jpg\n",
            ">47871819_db55ac4699.jpg\n",
            ">109202801_c6381eef15.jpg\n",
            ">111537217_082a4ba060.jpg\n",
            ">667626_18933d713e.jpg\n",
            ">109823394_83fcb735e1.jpg\n",
            ">109823395_6fb423a90f.jpg\n",
            ">70995350_75d0698839.jpg\n",
            ">109738763_90541ef30d.jpg\n",
            ">42637987_866635edf6.jpg\n",
            ">56489627_e1de43de34.jpg\n",
            ">109671650_f7bbc297fa.jpg\n",
            ">57417274_d55d34e93e.jpg\n",
            ">90011335_cfdf9674c2.jpg\n",
            ">107582366_d86f2d3347.jpg\n",
            ">58368365_03ed3e5bdf.jpg\n",
            ">57422853_b5f6366081.jpg\n",
            ">69710411_2cf537f61f.jpg\n",
            ">111497985_38e9f88856.jpg\n",
            ">109202756_b97fcdc62c.jpg\n",
            ">95783195_e1ba3f57ca.jpg\n",
            ">112243673_fd68255217.jpg\n",
            ">109260218_fca831f933.jpg\n",
            ">109260216_85b0be5378.jpg\n",
            ">3637013_c675de7705.jpg\n",
            ">49553964_cee950f3ba.jpg\n",
            ">86542183_5e312ae4d4.jpg\n",
            ">55473406_1d2271c1f2.jpg\n",
            ">54501196_a9ac9d66f2.jpg\n",
            ">110595925_f3395c8bd6.jpg\n",
            ">108898978_7713be88fc.jpg\n",
            ">17273391_55cfc7d3d4.jpg\n",
            ">53043785_c468d6f931.jpg\n",
            ">111537222_07e56d5a30.jpg\n",
            ">113678030_87a6a6e42e.jpg\n",
            ">108899015_bf36131a57.jpg\n",
            ">58357057_dea882479e.jpg\n",
            ">55135290_9bed5c4ca3.jpg\n",
            ">102351840_323e3de834.jpg\n",
            ">106514190_bae200f463.jpg\n",
            ">10815824_2997e03d76.jpg\n",
            ">72964268_d532bb8ec7.jpg\n",
            ">78984436_ad96eaa802.jpg\n",
            ">111766423_4522d36e56.jpg\n",
            ">106490881_5a2dd9b7bd.jpg\n",
            ">58363930_0544844edd.jpg\n",
            ">50030244_02cd4de372.jpg\n",
            ">112178718_87270d9b4d.jpg\n",
            ">103195344_5d2dc613a3.jpg\n",
            ">55470226_52ff517151.jpg\n",
            ">107318069_e9f2ef32de.jpg\n",
            ">69189650_6687da7280.jpg\n",
            ">86412576_c53392ef80.jpg\n",
            ">41999070_838089137e.jpg\n",
            ">12830823_87d2654e31.jpg\n",
            ">19212715_20476497a3.jpg\n",
            ">103106960_e8a41d64f8.jpg\n",
            ">104136873_5b5d41be75.jpg\n",
            ">103205630_682ca7285b.jpg\n",
            ">54723805_bcf7af3f16.jpg\n",
            ">105342180_4d4a40b47f.jpg\n",
            ">56494233_1824005879.jpg\n",
            ">35506150_cbdb630f4f.jpg\n",
            ">72218201_e0e9c7d65b.jpg\n",
            ">102455176_5f8ead62d5.jpg\n",
            ">109823397_e35154645f.jpg\n",
            ">109738916_236dc456ac.jpg\n",
            ">33108590_d685bfe51c.jpg\n",
            ">84713990_d3f3cef78b.jpg\n",
            ">58363928_6f7074608c.jpg\n",
            ">96985174_31d4c6f06d.jpg\n",
            ">44856031_0d82c2c7d1.jpg\n",
            ">101654506_8eb26cfb60.jpg\n",
            ">101669240_b2d3e7f17b.jpg\n",
            ">36422830_55c844bc2d.jpg\n",
            ">69710415_5c2bfb1058.jpg\n",
            "Extracted Features: 100\n",
            "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
            "Wall time: 10 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AklFYNkTsLr",
        "colab_type": "text"
      },
      "source": [
        "## Calling Text Data for Preprocessing and saving "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJq0vUrUTsLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e50512e2-269b-4efc-c78e-dcdeb36dd73c"
      },
      "source": [
        "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/text_file.txt'\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "clean_descriptions(descriptions)\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 99 \n",
            "Vocabulary Size: 827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5VG7cZTsMA",
        "colab_type": "text"
      },
      "source": [
        "## Calling for  Training the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLotKTU0TsMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f564f47b-fbed-4f64-8fa3-7494a9f85d69"
      },
      "source": [
        "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Train.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 80\n",
            "Descriptions: train=79\n",
            "Photos: train=80\n",
            "Vocabulary Size: 734\n",
            "Description Length: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hHFlE6fYiua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNQYFkR7TsMe",
        "colab_type": "text"
      },
      "source": [
        "## Setting Development Part "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF3Ttl8_TsMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a405e2e8-e840-45a5-f12d-04ea0cafd42c"
      },
      "source": [
        "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Dev.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 19\n",
            "Descriptions: test=19\n",
            "Photos: test=19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWhootymTsMv",
        "colab_type": "text"
      },
      "source": [
        "## Calling the Test Part "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRrlHC5LTsMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "13d4f33c-8800-4fbb-9a16-89bf374e162a"
      },
      "source": [
        "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Test.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 18\n",
            "Descriptions: test=18\n",
            "Photos: test=18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw2yZvuFTsM-",
        "colab_type": "text"
      },
      "source": [
        "## Calling Model function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQLJa1ETsNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c691415-ed8f-4993-82e1-1c0322a82afb"
      },
      "source": [
        "model = define_model(vocab_size, max_length)\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 21, 256)      187904      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 21, 256)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 734)          188638      dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,016,478\n",
            "Trainable params: 2,016,478\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.03888, saving model to model-ep001-loss5.808-val_loss5.039.h5\n",
            "99/99 - 14s - loss: 5.8079 - val_loss: 5.0389\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.03888 to 4.19232, saving model to model-ep002-loss4.851-val_loss4.192.h5\n",
            "99/99 - 12s - loss: 4.8509 - val_loss: 4.1923\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 4.19232 to 3.71897, saving model to model-ep003-loss4.230-val_loss3.719.h5\n",
            "99/99 - 12s - loss: 4.2304 - val_loss: 3.7190\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.71897 to 3.32421, saving model to model-ep004-loss3.830-val_loss3.324.h5\n",
            "99/99 - 12s - loss: 3.8298 - val_loss: 3.3242\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.32421 to 3.02113, saving model to model-ep005-loss3.524-val_loss3.021.h5\n",
            "99/99 - 12s - loss: 3.5242 - val_loss: 3.0211\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.02113 to 2.82669, saving model to model-ep006-loss3.282-val_loss2.827.h5\n",
            "99/99 - 12s - loss: 3.2820 - val_loss: 2.8267\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.82669 to 2.61085, saving model to model-ep007-loss3.071-val_loss2.611.h5\n",
            "99/99 - 12s - loss: 3.0713 - val_loss: 2.6108\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.61085 to 2.44997, saving model to model-ep008-loss2.858-val_loss2.450.h5\n",
            "99/99 - 12s - loss: 2.8579 - val_loss: 2.4500\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.44997 to 2.23152, saving model to model-ep009-loss2.656-val_loss2.232.h5\n",
            "99/99 - 12s - loss: 2.6564 - val_loss: 2.2315\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 2.23152 to 1.98999, saving model to model-ep010-loss2.466-val_loss1.990.h5\n",
            "99/99 - 12s - loss: 2.4656 - val_loss: 1.9900\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.98999 to 1.84618, saving model to model-ep011-loss2.248-val_loss1.846.h5\n",
            "99/99 - 12s - loss: 2.2485 - val_loss: 1.8462\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.84618 to 1.62670, saving model to model-ep012-loss2.062-val_loss1.627.h5\n",
            "99/99 - 12s - loss: 2.0622 - val_loss: 1.6267\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.62670 to 1.44796, saving model to model-ep013-loss1.882-val_loss1.448.h5\n",
            "99/99 - 12s - loss: 1.8818 - val_loss: 1.4480\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.44796 to 1.28495, saving model to model-ep014-loss1.695-val_loss1.285.h5\n",
            "99/99 - 12s - loss: 1.6950 - val_loss: 1.2849\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.28495 to 1.12566, saving model to model-ep015-loss1.551-val_loss1.126.h5\n",
            "99/99 - 12s - loss: 1.5509 - val_loss: 1.1257\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.12566 to 1.05867, saving model to model-ep016-loss1.390-val_loss1.059.h5\n",
            "99/99 - 12s - loss: 1.3897 - val_loss: 1.0587\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.05867 to 0.92241, saving model to model-ep017-loss1.260-val_loss0.922.h5\n",
            "99/99 - 12s - loss: 1.2598 - val_loss: 0.9224\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.92241 to 0.79108, saving model to model-ep018-loss1.112-val_loss0.791.h5\n",
            "99/99 - 12s - loss: 1.1122 - val_loss: 0.7911\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.79108 to 0.67760, saving model to model-ep019-loss0.983-val_loss0.678.h5\n",
            "99/99 - 12s - loss: 0.9834 - val_loss: 0.6776\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.67760 to 0.58973, saving model to model-ep020-loss0.859-val_loss0.590.h5\n",
            "99/99 - 12s - loss: 0.8588 - val_loss: 0.5897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90f5df7e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYGncYUTsNK",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8LI0S3TsNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "77923472-a858-431f-9075-17f6ba80a558"
      },
      "source": [
        "filename = 'model-ep020-loss0.859-val_loss0.590.h5'\n",
        "model = load_model(filename)\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.214646\n",
            "BLEU-2: 0.085919\n",
            "BLEU-3: 0.039225\n",
            "BLEU-4: 0.067293\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnQ4wi0HTsNX",
        "colab_type": "text"
      },
      "source": [
        "## Caption Generation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7-NBn_-bMnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_feature(filename):\n",
        "\tmodel = VGG16()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\timage = preprocess_input(image)\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpfZfq-vTsNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5d9d5f9f-67f4-4a03-b1fd-257c8aec202a"
      },
      "source": [
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "max_length = 34\n",
        "model = load_model('model-ep020-loss0.859-val_loss0.590.h5')\n",
        "photo = extract_feature('/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Images/101654506_8eb26cfb60.jpg')\n",
        "print(photo)\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.7495301  ... 0.         2.2005942  0.53506637]]\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 21) for input Tensor(\"input_6_8:0\", shape=(None, 21), dtype=float32), but it was called on an input with incompatible shape (None, 34).\n",
            "startseq and white and brown dog is running through the surface of the snow covered to snow covered the snow covered field the snow covered field the snow covered field the snow covered field the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3yNmbBaWulU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}