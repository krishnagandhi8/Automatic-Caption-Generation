{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "4vpn-PDlTzz2",
    "outputId": "00deafcd-d032-4c31-8a83-0d05e6d1cfbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrvN7P8yTsH4"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RM4bfRwqTsH8"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "import string\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghjU7VTWTsIO"
   },
   "source": [
    "## Extracting the Features using VGG-16 Model and storing it in Pickle File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mb9qbrLETsIQ"
   },
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    print(model.summary())\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be0EyDyWTsIb"
   },
   "source": [
    "## Loading the document with descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_c5nPm4TsId"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Skmufn5TsIn"
   },
   "source": [
    "## Loading Description and making it a dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oR33cdVkTsIp"
   },
   "outputs": [],
   "source": [
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsVCvzCOTsI0"
   },
   "source": [
    "## Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LF5NQfYgTsI2"
   },
   "outputs": [],
   "source": [
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] =  ' '.join(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3p-ZsmKmTsJA"
   },
   "source": [
    "## Converting loaded description into vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IRsUURFTsJC"
   },
   "outputs": [],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xMMTehHTsJM"
   },
   "source": [
    "## Saving Description to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YIXLBsSTsJO"
   },
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KCq0aV6TsJZ"
   },
   "source": [
    "# Training\n",
    "## load a pre-defined list of photo identifiers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2z3vogcxTsJb"
   },
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SUcpY2NTsJk"
   },
   "source": [
    "## Loading Preprocessed Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f26MwZegTsJm"
   },
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = ' '.join(image_desc)\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noYS2mlkTsJt"
   },
   "source": [
    "## Loading features from pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4whXCH9QTsJw"
   },
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pt_1hsexTsJ6"
   },
   "source": [
    "## Convert Dictionaries into list of string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGjm9HDfTsJ8"
   },
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KcZaNMATsKE"
   },
   "source": [
    "## Create tokens from list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWf7zre2TsKG"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8S7SR3ETsKQ"
   },
   "source": [
    "## To calculate maximum number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3CfHEwdTsKS"
   },
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQMhj3lqTsKb"
   },
   "source": [
    "## create sequences of images, input sequences and output words for an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t9S_taFTsKd"
   },
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bzm0204bTsKr"
   },
   "source": [
    "## Defining Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmHj9EdoTsKt"
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDDd57TcTsK6"
   },
   "source": [
    "## Converting Integer back into word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KOsZHNhTsK7"
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvMftnBSTsLG"
   },
   "source": [
    "## Generating a textual for an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCCkktyUTsLH"
   },
   "outputs": [],
   "source": [
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        yhat = argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LP4QFIQJTsLR"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmjnBHckTsLS"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jF2_ZFQSTsLe"
   },
   "source": [
    "## Calling VGG-16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-6d-YtxDTsLg",
    "outputId": "2a4ea251-26bc-4e25-e5a3-b55741adba8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      ">99171998_7cc800ceef.jpg\n",
      ">96420612_feb18fc6c6.jpg\n",
      ">93922153_8d831f7f01.jpg\n",
      ">96973080_783e375945.jpg\n",
      ">95728660_d47de66544.jpg\n",
      ">95151149_5ca6747df6.jpg\n",
      ">95734038_2ab5783da7.jpg\n",
      ">96978713_775d66a18d.jpg\n",
      ">95734035_84732a92c1.jpg\n",
      ">97105139_fae46fe8ef.jpg\n",
      ">97406261_5eea044056.jpg\n",
      ">97577988_65e2eae14a.jpg\n",
      ">94232465_a135df2711.jpg\n",
      ">95734036_bef6d1a871.jpg\n",
      ">99679241_adc853a5c0.jpg\n",
      ">95728664_06c43b90f1.jpg\n",
      ">44129946_9eeb385d77.jpg\n",
      ">27782020_4dab210360.jpg\n",
      ">97731718_eb7ba71fd3.jpg\n",
      ">42637986_135a9786a6.jpg\n",
      ">47870024_73a4481f7d.jpg\n",
      ">98377566_e4674d1ebd.jpg\n",
      ">23445819_3a458716c1.jpg\n",
      ">96399948_b86c61bfe6.jpg\n",
      ">61209225_8512e1dad5.jpg\n",
      ">47871819_db55ac4699.jpg\n",
      ">109202801_c6381eef15.jpg\n",
      ">111537217_082a4ba060.jpg\n",
      ">667626_18933d713e.jpg\n",
      ">109823394_83fcb735e1.jpg\n",
      ">109823395_6fb423a90f.jpg\n",
      ">70995350_75d0698839.jpg\n",
      ">109738763_90541ef30d.jpg\n",
      ">42637987_866635edf6.jpg\n",
      ">56489627_e1de43de34.jpg\n",
      ">109671650_f7bbc297fa.jpg\n",
      ">57417274_d55d34e93e.jpg\n",
      ">90011335_cfdf9674c2.jpg\n",
      ">107582366_d86f2d3347.jpg\n",
      ">58368365_03ed3e5bdf.jpg\n",
      ">57422853_b5f6366081.jpg\n",
      ">69710411_2cf537f61f.jpg\n",
      ">111497985_38e9f88856.jpg\n",
      ">109202756_b97fcdc62c.jpg\n",
      ">95783195_e1ba3f57ca.jpg\n",
      ">112243673_fd68255217.jpg\n",
      ">109260218_fca831f933.jpg\n",
      ">109260216_85b0be5378.jpg\n",
      ">3637013_c675de7705.jpg\n",
      ">49553964_cee950f3ba.jpg\n",
      ">86542183_5e312ae4d4.jpg\n",
      ">55473406_1d2271c1f2.jpg\n",
      ">54501196_a9ac9d66f2.jpg\n",
      ">110595925_f3395c8bd6.jpg\n",
      ">108898978_7713be88fc.jpg\n",
      ">17273391_55cfc7d3d4.jpg\n",
      ">53043785_c468d6f931.jpg\n",
      ">111537222_07e56d5a30.jpg\n",
      ">113678030_87a6a6e42e.jpg\n",
      ">108899015_bf36131a57.jpg\n",
      ">58357057_dea882479e.jpg\n",
      ">55135290_9bed5c4ca3.jpg\n",
      ">102351840_323e3de834.jpg\n",
      ">106514190_bae200f463.jpg\n",
      ">10815824_2997e03d76.jpg\n",
      ">72964268_d532bb8ec7.jpg\n",
      ">78984436_ad96eaa802.jpg\n",
      ">111766423_4522d36e56.jpg\n",
      ">106490881_5a2dd9b7bd.jpg\n",
      ">58363930_0544844edd.jpg\n",
      ">50030244_02cd4de372.jpg\n",
      ">112178718_87270d9b4d.jpg\n",
      ">103195344_5d2dc613a3.jpg\n",
      ">55470226_52ff517151.jpg\n",
      ">107318069_e9f2ef32de.jpg\n",
      ">69189650_6687da7280.jpg\n",
      ">86412576_c53392ef80.jpg\n",
      ">41999070_838089137e.jpg\n",
      ">12830823_87d2654e31.jpg\n",
      ">19212715_20476497a3.jpg\n",
      ">103106960_e8a41d64f8.jpg\n",
      ">104136873_5b5d41be75.jpg\n",
      ">103205630_682ca7285b.jpg\n",
      ">54723805_bcf7af3f16.jpg\n",
      ">105342180_4d4a40b47f.jpg\n",
      ">56494233_1824005879.jpg\n",
      ">35506150_cbdb630f4f.jpg\n",
      ">72218201_e0e9c7d65b.jpg\n",
      ">102455176_5f8ead62d5.jpg\n",
      ">109823397_e35154645f.jpg\n",
      ">109738916_236dc456ac.jpg\n",
      ">33108590_d685bfe51c.jpg\n",
      ">84713990_d3f3cef78b.jpg\n",
      ">58363928_6f7074608c.jpg\n",
      ">96985174_31d4c6f06d.jpg\n",
      ">44856031_0d82c2c7d1.jpg\n",
      ">101654506_8eb26cfb60.jpg\n",
      ">101669240_b2d3e7f17b.jpg\n",
      ">36422830_55c844bc2d.jpg\n",
      ">69710415_5c2bfb1058.jpg\n",
      "Extracted Features: 100\n",
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "directory = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Images'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "dump(features, open('features.pkl', 'wb'))\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AklFYNkTsLr"
   },
   "source": [
    "## Calling Text Data for Preprocessing and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "BJq0vUrUTsLt",
    "outputId": "e50512e2-269b-4efc-c78e-dcdeb36dd73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 99 \n",
      "Vocabulary Size: 827\n"
     ]
    }
   ],
   "source": [
    "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/text_file.txt'\n",
    "doc = load_doc(filename)\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "clean_descriptions(descriptions)\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yE5VG7cZTsMA"
   },
   "source": [
    "## Calling for  Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "NLotKTU0TsMC",
    "outputId": "f564f47b-fbed-4f64-8fa3-7494a9f85d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 80\n",
      "Descriptions: train=79\n",
      "Photos: train=80\n",
      "Vocabulary Size: 734\n",
      "Description Length: 21\n"
     ]
    }
   ],
   "source": [
    "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Train.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hHFlE6fYiua"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNQYFkR7TsMe"
   },
   "source": [
    "## Setting Development Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "oF3Ttl8_TsMg",
    "outputId": "a405e2e8-e840-45a5-f12d-04ea0cafd42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 19\n",
      "Descriptions: test=19\n",
      "Photos: test=19\n"
     ]
    }
   ],
   "source": [
    "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Dev.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TWhootymTsMv"
   },
   "source": [
    "## Calling the Test Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "qRrlHC5LTsMw",
    "outputId": "13d4f33c-8800-4fbb-9a16-89bf374e162a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 18\n",
      "Descriptions: test=18\n",
      "Photos: test=18\n"
     ]
    }
   ],
   "source": [
    "filename = '/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Text/Test.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cw2yZvuFTsM-"
   },
   "source": [
    "## Calling Model function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uXQLJa1ETsNA",
    "outputId": "9c691415-ed8f-4993-82e1-1c0322a82afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 21)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 21, 256)      187904      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 21, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 734)          188638      dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,016,478\n",
      "Trainable params: 2,016,478\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.03888, saving model to model-ep001-loss5.808-val_loss5.039.h5\n",
      "99/99 - 14s - loss: 5.8079 - val_loss: 5.0389\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.03888 to 4.19232, saving model to model-ep002-loss4.851-val_loss4.192.h5\n",
      "99/99 - 12s - loss: 4.8509 - val_loss: 4.1923\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.19232 to 3.71897, saving model to model-ep003-loss4.230-val_loss3.719.h5\n",
      "99/99 - 12s - loss: 4.2304 - val_loss: 3.7190\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.71897 to 3.32421, saving model to model-ep004-loss3.830-val_loss3.324.h5\n",
      "99/99 - 12s - loss: 3.8298 - val_loss: 3.3242\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.32421 to 3.02113, saving model to model-ep005-loss3.524-val_loss3.021.h5\n",
      "99/99 - 12s - loss: 3.5242 - val_loss: 3.0211\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.02113 to 2.82669, saving model to model-ep006-loss3.282-val_loss2.827.h5\n",
      "99/99 - 12s - loss: 3.2820 - val_loss: 2.8267\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.82669 to 2.61085, saving model to model-ep007-loss3.071-val_loss2.611.h5\n",
      "99/99 - 12s - loss: 3.0713 - val_loss: 2.6108\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.61085 to 2.44997, saving model to model-ep008-loss2.858-val_loss2.450.h5\n",
      "99/99 - 12s - loss: 2.8579 - val_loss: 2.4500\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.44997 to 2.23152, saving model to model-ep009-loss2.656-val_loss2.232.h5\n",
      "99/99 - 12s - loss: 2.6564 - val_loss: 2.2315\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.23152 to 1.98999, saving model to model-ep010-loss2.466-val_loss1.990.h5\n",
      "99/99 - 12s - loss: 2.4656 - val_loss: 1.9900\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.98999 to 1.84618, saving model to model-ep011-loss2.248-val_loss1.846.h5\n",
      "99/99 - 12s - loss: 2.2485 - val_loss: 1.8462\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.84618 to 1.62670, saving model to model-ep012-loss2.062-val_loss1.627.h5\n",
      "99/99 - 12s - loss: 2.0622 - val_loss: 1.6267\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.62670 to 1.44796, saving model to model-ep013-loss1.882-val_loss1.448.h5\n",
      "99/99 - 12s - loss: 1.8818 - val_loss: 1.4480\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.44796 to 1.28495, saving model to model-ep014-loss1.695-val_loss1.285.h5\n",
      "99/99 - 12s - loss: 1.6950 - val_loss: 1.2849\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.28495 to 1.12566, saving model to model-ep015-loss1.551-val_loss1.126.h5\n",
      "99/99 - 12s - loss: 1.5509 - val_loss: 1.1257\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.12566 to 1.05867, saving model to model-ep016-loss1.390-val_loss1.059.h5\n",
      "99/99 - 12s - loss: 1.3897 - val_loss: 1.0587\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.05867 to 0.92241, saving model to model-ep017-loss1.260-val_loss0.922.h5\n",
      "99/99 - 12s - loss: 1.2598 - val_loss: 0.9224\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.92241 to 0.79108, saving model to model-ep018-loss1.112-val_loss0.791.h5\n",
      "99/99 - 12s - loss: 1.1122 - val_loss: 0.7911\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.79108 to 0.67760, saving model to model-ep019-loss0.983-val_loss0.678.h5\n",
      "99/99 - 12s - loss: 0.9834 - val_loss: 0.6776\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.67760 to 0.58973, saving model to model-ep020-loss0.859-val_loss0.590.h5\n",
      "99/99 - 12s - loss: 0.8588 - val_loss: 0.5897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90f5df7e80>"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdYGncYUTsNK"
   },
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Pg8LI0S3TsNM",
    "outputId": "77923472-a858-431f-9075-17f6ba80a558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.214646\n",
      "BLEU-2: 0.085919\n",
      "BLEU-3: 0.039225\n",
      "BLEU-4: 0.067293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "filename = 'model-ep020-loss0.859-val_loss0.590.h5'\n",
    "model = load_model(filename)\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xnQ4wi0HTsNX"
   },
   "source": [
    "## Caption Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7-NBn_-bMnw"
   },
   "outputs": [],
   "source": [
    "def extract_feature(filename):\n",
    "    model = VGG16()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "zpfZfq-vTsNZ",
    "outputId": "5d9d5f9f-67f4-4a03-b1fd-257c8aec202a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.7495301  ... 0.         2.2005942  0.53506637]]\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 21) for input Tensor(\"input_6_8:0\", shape=(None, 21), dtype=float32), but it was called on an input with incompatible shape (None, 34).\n",
      "startseq and white and brown dog is running through the surface of the snow covered to snow covered the snow covered field the snow covered field the snow covered field the snow covered field the\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "max_length = 34\n",
    "model = load_model('model-ep020-loss0.859-val_loss0.590.h5')\n",
    "photo = extract_feature('/content/drive/My Drive/Trimester 4-20200920T052747Z-001/Trimester 4/Images/101654506_8eb26cfb60.jpg')\n",
    "print(photo)\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3yNmbBaWulU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Automatic Image Captioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
